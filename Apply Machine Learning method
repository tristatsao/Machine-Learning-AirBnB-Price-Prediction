#knn
#training X &Y
#check where's the location of review_scores_rating colnames(training)
colnames(test)
train_x = training[, -1] train_x = scale(train_x)[,] train_y = training[,1] #test X &Y
test_x = test[, -1]
test_x = scale(test[,-1])[,] test_y = test[,1]
#knn model test
knnmodel = knnreg(train_x, train_y)
#predict
pred_y = predict(knnmodel, data.frame(test_x))
print(data.frame(test_y, pred_y)) R2(pred_y,test_y,form="traditional") mse = mean((test_y - pred_y)^2) mae = caret::MAE(test_y, pred_y) rmse = caret::RMSE(test_y, pred_y)
cat("MSE: ", mse, "MAE: ", mae, " RMSE: ", rmse)
x = 1:length(test_y)
plot(x[1:50], test_y[1:50], col = "red", type = "l", lwd=2,
main = "knn regression: test data prediction")
lines(x, pred_y, col = "blue", lwd=2) legend("bottomright", legend = c("original", "predicted"),
fill = c("red", "blue"), col = 2:3, adj = c(0, 0.6)) grid()
############################################################# #train
knnmodel = knnreg(train_x, train_y)
#predict
pred_y = predict(knnmodel, data.frame(train_x)) print(data.frame(train_y, pred_y))
 R2(pred_y,train_y,form="traditional") mse = mean((train_y - pred_y)^2) mae = caret::MAE(train_y, pred_y) rmse = caret::RMSE(train_y, pred_y)
cat("MSE: ", mse, "MAE: ", mae, " RMSE: ", rmse)
x = 1:length(train_y)
plot(x[1:50], train_y[1:50], col = "red", type = "l", lwd=2,
main = "knn regression: train data prediction")
lines(x, pred_y, col = "blue", lwd=2) legend("bottomright", legend = c("original", "predicted"),
fill = c("red", "blue"), col = 2:3, adj = c(0, 0.6)) grid()
linear regression
####################### #Scaling the features## #######################
scaled_train = training scaled_test = test
#only the predictors
cols = c("review_scores_value", 'review_scores_communication',
'host_is_superhost', 'host_total_listings_count', 'price',
'number_of_reviews', 'privateroom', 'accommodates', 'is_apartment', 'review_scores_checkin', 'minimum_nights', 'lock_on_bedroom_door',
'host_response_rate', 'pool', 'availability_365', 'maximum_nights', 'hollywood_neighbourhood', 'loft',
'bathrooms', 'review_scores_location', 'recent_review', 'reviews_per_month','guesthouse','santa_monica_neighbourhood',
'venice_neighbourhood','elevator_in_building')
pre_proc_val <- preProcess(scaled_train[,cols], method = c("center", "scale"))
scaled_train[,cols] = predict(pre_proc_val, scaled_train[,cols]) scaled_test[,cols] = predict(pre_proc_val, scaled_test[,cols])
summary(scaled_train)
############################### #standard multiple regression## ###############################

 lr = lm(review_scores_rating ~ review_scores_value + review_scores_communication + host_is_superhost +
host_total_listings_count + price+number_of_reviews + privateroom + accommodates + is_apartment + review_scores_checkin+minimum_nights+ lock_on_bedroom_door + host_response_rate + pool + availability_365 + maximum_nights +
hollywood_neighbourhood +
loft + bathrooms + review_scores_location + recent_review + reviews_per_month +
guesthouse +
santa_monica_neighbourhood + venice_neighbourhood + elevator_in_building, data =
scaled_train) summary(lr)
#Evaluation : Type 1
reg_pred = predict(lr, newdata = scaled_train)
MAE(reg_pred,training$review_scores_rating) mse(reg_pred,training$review_scores_rating) rmse(reg_pred,training$review_scores_rating)
#Evaluation : Type 2
#Step 1 - creating the evaluation metrics function eval_metrics = function(model, df, predictions, target){
resids = df[,target] - predictions
resids2 = resids**2
N = length(predictions)
r2 = as.character(round(summary(model)$r.squared, 2))
adj_r2 = as.character(round(summary(model)$adj.r.squared, 2)) print(adj_r2) #Adjusted R-squared print(as.character(round(sqrt(sum(resids2)/N), 2))) #RMSE
}
# Step 2 - predicting and evaluating the model on train data
predictions = predict(lr, newdata = scaled_train)
eval_metrics(lr, scaled_train, predictions, target = 'review_scores_rating')
# 0.44 # 4.98
# TRAINING
# Multiple R-squared: 0.4432,
#AdjustedR-squared: 0.4424
# MAE 3.161168
# MSE 24.78808
# RMSE 4.978763/ 4.98
# The above output shows that RMSE, one of the two evaluation metrics, # is 4.98 for training data. On the other hand, R-squared value is around # 44% for training data which indicates average performance.

 #Test
cl = training$review_scores_rating cl_test = test$review_scores_rating
res2 = lm(review_scores_rating ~ review_scores_value + review_scores_communication + host_is_superhost +
host_total_listings_count + price+number_of_reviews + privateroom + accommodates +
is_apartment + review_scores_checkin+minimum_nights+ lock_on_bedroom_door +
host_response_rate + pool + availability_365 + maximum_nights + hollywood_neighbourhood +
loft + bathrooms + review_scores_location + recent_review + reviews_per_month + guesthouse +
santa_monica_neighbourhood + venice_neighbourhood + elevator_in_building, data = scaled_test)
summary(res2)
#Evaluation : Type 1
reg_pred2 = predict(res2, newdata = scaled_test)
MAE(reg_pred2,test$review_scores_rating) mse(reg_pred2,test$review_scores_rating) rmse(reg_pred2,test$review_scores_rating)
#Evaluation : Type 2
#Step 1 - creating the evaluation metrics function eval_metrics = function(model, df, predictions, target){
resids = df[,target] - predictions
resids2 = resids**2
N = length(predictions)
r2 = as.character(round(summary(model)$r.squared, 2))
adj_r2 = as.character(round(summary(model)$adj.r.squared, 2)) print(adj_r2) #Adjusted R-squared print(as.character(round(sqrt(sum(resids2)/N), 2))) #RMSE
}
# Step 2 - predicting and evaluating the model on test data
predictions = predict(lr, newdata = scaled_test)
eval_metrics(lr, scaled_test, predictions, target = 'review_scores_rating')
# "0.44" # "5.13"
#The above output shows that RMSE, one of the two evaluation metrics, #is 5.13 for test data. On the other hand, R-squared value is around

 # 44 % for test data which indicates average performance also lower than the training data.
#Test
# Multiple R-squared: 0.4251 #AdjustedR-squared: 0.4217 # MAE 3.220544
# MSE 26.12188
# RMSE 5.110957/ 5.13
##Regularization##
cols_reg = c('review_scores_rating','review_scores_value', 'review_scores_communication', 'host_is_superhost', 'host_total_listings_count', 'price',
'number_of_reviews', 'privateroom', 'accommodates', 'is_apartment', 'review_scores_checkin', 'minimum_nights', 'lock_on_bedroom_door', 'host_response_rate', 'pool', 'availability_365', 'maximum_nights', 'hollywood_neighbourhood', 'loft','bathrooms', 'review_scores_location', 'recent_review', 'reviews_per_month','guesthouse','santa_monica_neighbourhood', 'venice_neighbourhood','elevator_in_building')
dummies <- dummyVars(review_scores_rating ~ ., data = LA_sample[,cols_reg]) train_dummies = predict(dummies, newdata = training[,cols_reg])
test_dummies = predict(dummies, newdata = test[,cols_reg]) print(dim(train_dummies)); print(dim(test_dummies))
####################### ###Ridge Regression ### #######################
library(glmnet) library(MASS)
cl = training$review_scores_rating cl_test = test$review_scores_rating
x = as.matrix(train_dummies) y_train = cl
x_test = as.matrix(test_dummies) y_test = cl_test
lambdas <- 10^seq(2, -3, by = -.1)
ridge_reg = glmnet(x, y_train, nlambda = 25, alpha = 0, family = 'gaussian', lambda = lambdas)

 summary(ridge_reg)
coef(ridge_reg) plot(ridge_reg)
cv_ridge <- cv.glmnet(x, y_train, alpha = 0, lambda = lambdas) optimal_lambda <- cv_ridge$lambda.min
optimal_lambda
# Compute R^2 from true and predicted values eval_results <- function(true, predicted, df) {
SSE <- sum((predicted - true)^2) SST <- sum((true - mean(true))^2) R_square <- 1 - SSE / SST
RMSE = sqrt(SSE/nrow(df))
# Model performance metrics data.frame(
RMSE = RMSE,
Rsquare = R_square )
}
# Prediction and evaluation on train data
predictions_train <- predict(ridge_reg, s = optimal_lambda, newx = x) eval_results(y_train, predictions_train, scaled_train) R2(predictions_train,scaled_train$review_scores_rating)
# Prediction and evaluation on test data
predictions_test <- predict(ridge_reg, s = optimal_lambda, newx = x_test) eval_results(y_test, predictions_test, scaled_test) R2(predictions_test,scaled_test$review_scores_rating)
#TRAINING # RMSE
# 6.284161
# Rsquare
# 0.396136
# TEST # RMSE #6.35103

 # 0.3710907
####################### ###Lasso Regression ### ####################### x = as.matrix(train_dummies) y_train = cl
x_test = as.matrix(test_dummies) y_test = cl_test
lambdas <- 10^seq(2, -3, by = -.1)
# Setting alpha = 1 implements lasso regression
lasso_reg <- cv.glmnet(x, y_train, alpha = 1, lambda = lambdas, standardize = TRUE, nfolds = 25)
# Best
lambda_best <- lasso_reg$lambda.min lambda_best
lasso_model <- glmnet(x, y_train, alpha = 1, lambda = lambda_best, standardize = TRUE)
# Compute R^2 from true and predicted values eval_results <- function(true, predicted, df) {
SSE <- sum((predicted - true)^2) SST <- sum((true - mean(true))^2) R_square <- 1 - SSE / SST
RMSE = sqrt(SSE/nrow(df))
# Model performance metrics data.frame(
RMSE = RMSE,
Rsquare = R_square )
}
predictions_train <- predict(lasso_model, s = lambda_best, newx = x) eval_results(y_train, predictions_train, scaled_train)
predictions_test <- predict(lasso_model, s = lambda_best, newx = x_test) eval_results(y_test, predictions_test, scaled_test)
# TRAIN # RMSE
# 4.980063

 # Rsquare
# 0.4429111
# TEST
# RMSE
# 5.12923
# Rsquare
# 0.4209481
############################## ###Elastic Net Regression #### ############################## install.packages("repr")
library(repr)
# Set training control
train_cont <- trainControl(method = "repeatedcv",
number = 10,
repeats = 5,
search = "random", verboseIter = TRUE)
# Train the model
elastic_reg <- train(review_scores_rating~.,
data = training,
method = "glmnet",
preProcess = c("center", "scale"), tuneLength = 10,
trControl = train_cont)
# Best tuning parameter elastic_reg$bestTune
x = as.matrix(train_dummies) y_train = cl
x_test = as.matrix(test_dummies) y_test = cl_test
# Make predictions on training set predictions_train <- predict(elastic_reg, x) eval_results(y_train, predictions_train, training)
# Make predictions on test set predictions_test <- predict(elastic_reg, x_test) eval_results(y_test, predictions_test, test)

 #TRAINING # RMSE
# 5.006063
# Rsquare
# 0.4370789
# TEST
# RMSE
# 5.145082 # Rsquare
# 0.4173635
SVM
library(e1071)
library(raster)
library(rgeos)
model_svm = svm(review_scores_rating ~review_scores_value+ review_scores_communication+
host_is_superhost+ host_total_listings_count+ price+
number_of_reviews+ privateroom+ accommodates+ is_apartment+ review_scores_checkin+ minimum_nights+ lock_on_bedroom_door+
host_response_rate+ pool+ availability_365+ maximum_nights+ hollywood_neighbourhood+ loft+
bathrooms+ review_scores_location+ recent_review+ reviews_per_month+guesthouse+santa_monica_neighbourhood+
venice_neighbourhood+elevator_in_building, data = training)
#model_1
pred_m1= predict(model_svm, training)
#check performance R2(pred_m1,training$review_scores_rating,form="traditional") MAE(pred_m1,training$review_scores_rating) mltools::mse(pred_m1,training$review_scores_rating) RMSE(pred_m1,training$review_scores_rating)
#model_test
pred_m1_t= predict(model_svm, test)
#check performance R2(pred_m1_t,test$review_scores_rating,form="traditional") MAE(pred_m1_t,test$review_scores_rating) mltools::mse(pred_m1_t,test$review_scores_rating) RMSE(pred_m1_t,test$review_scores_rating)

 #tune the model
svm_tune_1 <- svm(review_scores_rating ~ . ,
data = training, cost = 15, elsilon=0.1)
summary(svm_tune_1)
#predict model tuned
pred_m2= predict(svm_tune_1, training)
R2(pred_m2,training$review_scores_rating,form="traditional") MAE(pred_m2,training$review_scores_rating) mltools::mse(pred_m2,training$review_scores_rating) RMSE(pred_m2,training$review_scores_rating)
#tune 1 on training #R2:0.7776621#MAE:1.685361#MSE:9.898256#RMSE:3.146149 svm_tune_1 <- svm(review_scores_rating ~ . ,
data = training, cost = 15, elsilon=0.1)
pred_m2= predict(svm_tune_1, training)
R2(pred_m2,training$review_scores_rating,form="traditional") MAE(pred_m2,training$review_scores_rating) mltools::mse(pred_m2,training$review_scores_rating) RMSE(pred_m2,training$review_scores_rating)
#tune 1 on test #R2:0.5343584#MAE:2.766486#MSE:21.15625#RMSE:4.599592 pred_m2_t= predict(svm_tune_1, test)
R2(pred_m2_t,test$review_scores_rating,form="traditional") MAE(pred_m2_t,test$review_scores_rating) mltools::mse(pred_m2_t,test$review_scores_rating) RMSE(pred_m2_t,test$review_scores_rating)
#tune 2
svm_tune_2<- svm(review_scores_rating ~ . , data = training,
cost = 10, elsilon=0.1)

 #tune 2 on train
#R2:0.7620401#MAE:1.766324#MSE: 10.59373#RMSE:3.254801 pred_m3= predict(svm_tune_2, training)
R2(pred_m3,training$review_scores_rating,form="traditional") MAE(pred_m3,training$review_scores_rating) mltools::mse(pred_m3,training$review_scores_rating) RMSE(pred_m3,training$review_scores_rating)
#tune 2 on test #R2:0.5466223#MAE:2.720335#MSE:20.59904#RMSE:4.538617 pred_m3_t= predict(svm_tune_2, test)
R2(pred_m3_t,test$review_scores_rating,form="traditional") MAE(pred_m3_t,test$review_scores_rating) mltools::mse(pred_m3_t,test$review_scores_rating) RMSE(pred_m3_t,test$review_scores_rating)
#tune 3
svm_tune_3<- svm(review_scores_rating ~ . ,
data = training, cost = 5, elsilon=0.1)
#tune 3 on train #R2:0.7346685#MAE:1.907383#MSE:11.81228#RMSE:3.4369 pred_m4= predict(svm_tune_3, training)
R2(pred_m4,training$review_scores_rating,form="traditional") MAE(pred_m4,training$review_scores_rating) mltools::mse(pred_m4,training$review_scores_rating) RMSE(pred_m4,training$review_scores_rating)
#tune 3 on test #R2:0.563347#MAE:2.656166#MSE:19.83916#RMSE: 4.454117 pred_m4_t= predict(svm_tune_3, test)
R2(pred_m4_t,test$review_scores_rating,form="traditional") MAE(pred_m4_t,test$review_scores_rating) mltools::mse(pred_m4_t,test$review_scores_rating) RMSE(pred_m4_t,test$review_scores_rating)
Regression Tree
####################### ### Regression Tree ### #######################
library(rpart) #for fitting decision trees

 library(rpart.plot) #for plotting decision trees
# set.seed(1)
# train_rows <- sample(nrow(training),.7*nrow(training)) data_train <- training
data_valid <- test
tree <- rpart(review_scores_rating ~ ., data=data_train, control=rpart.control(cp=.0001))
#view results printcp(tree)
#identify best cp value to use
best <- tree$cptable[which.min(tree$cptable[,"xerror"]),"CP"] #produce a pruned tree based on the best cp value pruned_tree <- prune(tree, cp=best)
#use pruned tree to predict salary of this player
pred <- predict(pruned_tree, newdata=data_valid) train_pred <-predict(pruned_tree, newdata=data_train) library(Metrics)
rsq <- function(x, y) summary(lm(y~x))$r.squared rmse(pred, data_valid$review_scores_rating) MAE(pred, data_valid$review_scores_rating) mse(pred, data_valid$review_scores_rating) rsq(pred, data_valid$review_scores_rating)
rmse(train_pred, data_train$review_scores_rating) MAE(train_pred, data_train$review_scores_rating) mse(train_pred, data_train$review_scores_rating) rsq(train_pred, data_train$review_scores_rating)
Training: R2: 0.5669277, MAE: 2.826266, MSE: 19.68255, RMSE: 4.436502 Testing: R2: 0.5843472, MAE: 2.803829, MSE: 18.50444, RMSE: 4.301678
Random Forest
library(randomForest)
rffit <- randomForest(review_scores_rating ~ review_scores_value + review_scores_communication +
host_is_superhost + host_total_listings_count + price +
number_of_reviews + privateroom + accommodates + is_apartment + review_scores_checkin + minimum_nights + lock_on_bedroom_door +
host_response_rate + pool + availability_365 + maximum_nights + hollywood_neighbourhood + loft +

 bathrooms + review_scores_location + recent_review + reviews_per_month + guesthouse + santa_monica_neighbourhood +
venice_neighbourhood + elevator_in_building , data = training, importance = TRUE, na.action=na.omit, ntree = 50)
plot(rffit)
print(rffit)
varImpPlot(rffit)
#predict on the training data for training dataset predict_train_rf=predict(rffit, training)
#predict on the training data for test dataset predict_test_rf=predict(rffit, test)
#R square, MAE, MSE, RMSE-training R2(predict_train_rf,training$review_scores_rating,form="traditional") MAE(predict_train_rf,training$review_scores_rating) mltools::mse(predict_train_rf,training$review_scores_rating) #MSE(predict_train_rf,training$review_scores_rating) RMSE(predict_train_rf,training$review_scores_rating)
#R square, MAE, MSE, RMSE-test R2(predict_test_rf,test$review_scores_rating,form="traditional") MAE(predict_test_rf,test$review_scores_rating) mltools::mse(predict_test_rf,test$review_scores_rating) #MSE(predict_train_rf,training$review_scores_rating) RMSE(predict_test_rf,test$review_scores_rating)
Gradient Boosting Machine (GBM)
#######################
###Gradient Boosting Machine (GBM) ### #######################
set.seed(13343)
trControl = trainControl(method="cv",number=5)
tuneGrid = expand.grid(n.trees = 500, #500 notes -branches
interaction.depth = c(3), # 3 layers
shrinkage = (1:100)*0.001, # 1-100 ways, every way x 0.001. = 0.001 runs to 0.1 n.minobsinnode=c(10,15)) #10 or 15 the rial of a branch each branch least have 10 or
15 data points
#run the trainning model into cvmodel
garbage = capture.output(cvModel <- train(review_scores_rating~.,
data=training, method="gbm", trControl=trControl, tuneGrid=tuneGrid))
#avoid the review too messy

 #######
set.seed(13343)
cvboost = gbm(review_scores_rating~.,
data=training,
distribution="gaussian",
n.trees=500, interaction.depth=cvModel$bestTune$interaction.depth,
cvmodel by function bestTune shrinkage=cvModel$bestTune$shrinkage, n.minobsinnode = cvModel$bestTune$n.minobsinnode)
#know which is the best parameter cvModel$bestTune$interaction.depth cvModel$bestTune$shrinkage cvModel$bestTune$n.minobsinnode
#call out the best parameter of
#Now we can predict
pred_train = predict(cvboost, n.trees=500)
rmse_train_cv_boost = sqrt(mean((pred_train - training$review_scores_rating)^2)); rmse_train_cv_boost
R2(pred_train,training$review_scores_rating,form="traditional") MAE(pred_train,training$review_scores_rating) mltools::mse(pred_train,training$review_scores_rating)
pred = predict(cvboost, newdata = test, n.trees = 500)
rmse_cv_boost = sqrt(mean((pred - test$review_scores_rating)^2)); rmse_cv_boost R2(pred,test$review_scores_rating,form="traditional") MAE(pred,test$review_scores_rating) mltools::mse(pred,test$review_scores_rating)
#############GBM ways 2############################ #GB boost
cvboost_1 = gbm(review_scores_rating~.,data=training,
distribution = "gaussian", n.trees = 100, shrinkage = 0.1, interaction.depth = 3, bag.fraction = 0.5, train.fraction = 0.5, n.minobsinnode = 10, cv.folds = 5, keep.data = TRUE, verbose = FALSE, n.cores = 1)
# Check performance using the out-of-bag (OOB) error; the OOB error typically # underestimates the optimal number of iterations
best.iter <- gbm.perf(cvboost_1, method = "OOB")
print(best.iter)
#make a prediction on training

pre_gbm <- predict(cvboost_1, newdata = training, n.trees = best.iter, type = "link")
#performance R2(pre_gbm,training$review_scores_rating,form="traditional") MAE(pre_gbm,training$review_scores_rating) mltools::mse(pre_gbm,training$review_scores_rating) RMSE(pre_gbm,training$review_scores_rating)
#predict on test
pre_gbm_t <- predict(cvboost_1, newdata = test, n.trees = best.iter, type = "link")
#performance R2(pre_gbm_t,test$review_scores_rating,form="traditional") MAE(pre_gbm_t,test$review_scores_rating) mltools::mse(pre_gbm_t,test$review_scores_rating) RMSE(pre_gbm_t,test$review_scores_rating)
